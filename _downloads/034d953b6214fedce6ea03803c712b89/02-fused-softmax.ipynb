{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Fused Softmax\nIn this tutorial, you will write a fused softmax layer that outperform's PyTorch implementation and learn about:\n\n- The benefits of kernel fusion for bandwidth-bound operations.\n- The syntax and usage of reduction operators in Triton.\n- The automatic vectorization capabilities of the Triton compiler.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Motivations\nCustom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.\nLet us consider instead the case of a simple (numerically stabilized) softmax operation:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\n\n# Compute the row-wise softmax of x\ndef naive_softmax(x):\n    # read  MN elements ; write M  elements\n    x_max = torch.max(x, axis=1)[0]\n    # read 2MN elements ; write MN elements\n    z = x - x_max[:, None]\n    # read  MN elements ; write MN elements\n    numerator = torch.exp(x)\n    # read  MN elements ; write M  elements\n    denominator = torch.sum(numerator, axis=1)\n    # read 2MN elements ; write MN elements\n    ret = numerator / denominator[:, None]\n    # in total: read 7MN elements ; wrote 3MN + 2M elements\n    return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When implemented naively in pytorch, computing :code:`y = naive_softmax(x)` for $x \\in R^{M \\times N}$ requires reading $7MN$ elements from DRAM and writing back $3MN + 2M$ elements.\nInstead, we want to write a custom \"fused\" pytorch operators that only reads X once and does all the necessary computations on-chip.\nThis would require reading and writing back only $MN$ bytes, so we could expect a theoretical speed-up of 5x.\nIn practice, though, we expect less because our kernel will spend some time computing exponentials and moving data around in shared memory.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Kernel\nOur softmax kernel works as follows: each program loads a row of X and writes back a normalized row of Y. Note that one important limitation of Triton is that each block must have a power-of-two number of elements, which means that we need to guard the memory operations properly if we want to handle any possible input shapes:\n\n .. code-block:: C\n\n   __global__ void softmax(float* Y, float* X, int stride_xm, int stride_ym, int M, int N){\n     // row index\n     int    m             = get_program_id(0);\n     // column indices\n     int    n    [BLOCK] = 0 ... BLOCK;\n     // the memory address of all the elements\n     // that we want to load can be computed as follows\n     float* px   [BLOCK] = X + m*stride_xm + n;\n     // because BLOCK has to be a power of two\n     // (per Triton-C specs), it is important\n     // to guard each memory operation with predicates\n     // or we will read out of bounds\n     bool   check[BLOCK] = n < N;\n     float  x    [BLOCK] = check ? *px : -F32_INFINITY;\n     // syntax for reduction in Triton is:\n     // x[..., OPERATOR, ...]\n     //            ^\n     //           index\n     // The operators currently supported are {min, max, +}\n     float  z    [BLOCK] = x - x[max];\n     // The exponential in Triton is fast but approximate\n     // (i.e., like __expf in CUDA)\n     float  num  [BLOCK] = exp(z);\n     float  denom         = num[+];\n     // The result of the reduction is now stored in y\n     float  y    [BLOCK] = num / denom;\n     // We write it back\n     float* py   [BLOCK] = Y + m*stride_ym + n;\n     *?(check)py = y;\n   }\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Torch Bindings\nWe need to make sure that BLOCK is the smallest power of two\ngreater than the number of rows N of the input matrix.\nDifferent values of BLOCK will result in different kernels\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport triton\n\n# Source code for the Triton kernel\n_src = \"\"\"\n__global__ void softmax(float* Y, float* X, int stride_ym, int stride_xm, int M, int N){\n    int    m             = get_program_id(0);\n    int    n    [BLOCK] = 0 ... BLOCK;\n    float* px   [BLOCK] = X + m*stride_xm + n;\n    bool   check[BLOCK] = n < N;\n    float  x    [BLOCK] = check ? *px : -F32_INFINITY;\n    float  z    [BLOCK] = x - x[max];\n    float  num  [BLOCK] = exp(z);\n    float  denom        = num[+];\n    float  y    [BLOCK] = num / denom;\n    float* py   [BLOCK] = Y + m*stride_ym + n;\n    *?(check)py = y; \n}\n\"\"\"\n\n\ndef next_power_of_2(n):\n    n -= 1\n    n |= n >> 1\n    n |= n >> 2\n    n |= n >> 4\n    n |= n >> 8\n    n |= n >> 16\n    n += 1\n    return n\n\n\n_kernels = dict()\n\n\ndef make_kernel(N, device):\n    BLOCK = next_power_of_2(N)\n    key = (BLOCK, device)\n    if key not in _kernels:\n        defines = {'BLOCK': BLOCK}\n        _kernels[key] = triton.kernel(_src, device=device, defines=defines)\n    return _kernels[key]\n\n\nclass _softmax(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        # constraints of the op\n        assert x.dtype == torch.float32\n        y = torch.empty_like(x)\n        # *create launch grid*:\n        # here we just launch a grid of M programs\n        M, N = y.shape\n        grid = lambda opt: (M, )\n        # *launch kernel*:\n        kernel = make_kernel(N, y.device)\n        kernel(y.data_ptr(), x.data_ptr(), y.stride(0), x.stride(0), M, N, grid=grid)\n        return y\n\n\nsoftmax = _softmax.apply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unit Test\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.randn(1823, 781, device='cuda')\ny_tri = softmax(x)\ny_ref = torch.softmax(x, axis=1)\nprint(y_tri)\nprint(y_ref)\nprint(torch.allclose(y_tri, y_ref))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Seems to work!\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmarking\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nM = 4096\nNs = [128 * i for i in range(2, 50)]\ntri_ms = []\nref_ms = []\ndef_ms = []\nfor N in Ns:\n    x = torch.randn(M, N, device='cuda', dtype=torch.float32)\n    gbps = lambda ms: x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n    tri_ms += [gbps(triton.testing.do_bench(lambda: softmax(x)))]\n    ref_ms += [gbps(triton.testing.do_bench(lambda: torch.softmax(x, axis=1)))]\n    def_ms += [gbps(triton.testing.do_bench(lambda: naive_softmax(x)))]\nplt.xlabel('N')\nplt.ylabel('Bandwidth (GB/s)')\nplt.plot(Ns, tri_ms, label='Triton')\nplt.plot(Ns, ref_ms, label='Torch')\nplt.plot(Ns, def_ms, label='Naive')\nplt.legend()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}