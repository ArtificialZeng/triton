
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-60

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 61-69

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 71-80

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 80-110

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 111-112

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 112-186

.. code-block:: Python


    device = torch.cuda.current_device()
    properties = driver.active.utils.get_device_properties(device)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software piepling stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel, num_programs = kernels.get(BLOCK_SIZE, (None, 0))
        if kernel is None:
            kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                           num_stages=num_stages, num_warps=num_warps, grid=(1, ))
            kernel._init_handles()
            n_regs = kernel.n_regs
            size_smem = kernel.metadata.shared
            if is_hip():
                # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
                # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
                # ISA SECTION (3.6.4 for CDNA3)
                # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
                # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
                # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
                # not required to be equal numbers of both types.
                if is_cdna():
                    NUM_GPRS = NUM_REGS * 2

                # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
                # When we divide this number with WARP_SIZE we get maximum number of waves that can
                # execute on a CU (multi-processor)  in parallel.
                MAX_NUM_THREADS = properties["max_threads_per_sm"]
                max_num_waves = MAX_NUM_THREADS // WARP_SIZE
                occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
            else:
                occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
            occupancy = min(occupancy, SIZE_SMEM // size_smem)
            num_programs = NUM_SM * occupancy
            kernels[BLOCK_SIZE] = (kernel, num_programs)

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](
            y,
            x,
            x.stride(0),
            y.stride(0),
            n_rows,
            n_cols,
        )
        return y









.. GENERATED FROM PYTHON SOURCE LINES 187-189

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 191-193

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 193-200

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device='cuda')
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 201-202

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 204-209

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 209-240

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch'],  # possible values for `line_arg``
            line_names=[
                "Triton",
                "Torch",
            ],  # label name for the lines
            styles=[('blue', '-'), ('green', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device='cuda', dtype=torch.float32)
        stream = torch.cuda.Stream()
        torch.cuda.set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch
    0     256.0   479.444454   686.872743
    1     384.0   616.023791   819.110723
    2     512.0   757.426879   929.974025
    3     640.0   794.158863   950.787803
    4     768.0   881.783219  1027.350146
    5     896.0   935.064580  1061.095757
    6    1024.0   984.248144  1108.639908
    7    1152.0  1111.926451   613.882227
    8    1280.0  1138.590293   670.272194
    9    1408.0  1163.510052   726.009512
    10   1536.0  1195.454329   781.011664
    11   1664.0  1222.952270   817.542224
    12   1792.0  1235.422590   860.406771
    13   1920.0  1254.783114   905.928604
    14   2048.0  1279.870577   960.602133
    15   2176.0  1266.132413   973.668548
    16   2304.0  1267.679031  1010.307182
    17   2432.0  1291.997235  1053.691535
    18   2560.0  1299.243702  1086.289239
    19   2688.0  1314.737976  1103.980428
    20   2816.0  1320.632161  1127.300834
    21   2944.0  1330.232667  1168.475602
    22   3072.0  1344.083396  1182.906539
    23   3200.0  1348.502759  1195.974949
    24   3328.0  1354.273306  1226.486519
    25   3456.0  1377.774296  1245.017460
    26   3584.0  1379.364426  1258.274219
    27   3712.0  1386.017430  1275.093098
    28   3840.0  1384.815659  1297.013218
    29   3968.0  1389.623785  1318.781384
    30   4096.0  1397.064222  1324.406604
    31   4224.0  1334.592574  1159.760485
    32   4352.0  1330.570352  1176.833033
    33   4480.0  1355.056351  1185.424687
    34   4608.0  1363.345269  1196.101919
    35   4736.0  1356.484975  1197.241798
    36   4864.0  1375.787791  1219.214198
    37   4992.0  1372.238283  1237.340224
    38   5120.0  1370.585291  1248.636644
    39   5248.0  1375.429362  1258.580851
    40   5376.0  1377.727660  1287.898530
    41   5504.0  1377.742996  1299.684674
    42   5632.0  1383.267981  1315.363279
    43   5760.0  1398.087235  1326.805643
    44   5888.0  1391.640422  1344.209040
    45   6016.0  1403.148719  1357.306448
    46   6144.0  1410.042213  1372.709814
    47   6272.0  1414.245809  1376.404380
    48   6400.0  1414.054168  1387.733322
    49   6528.0  1416.404602  1392.287672
    50   6656.0  1418.942952  1400.681780
    51   6784.0  1409.546116  1416.139720
    52   6912.0  1429.045700  1425.332356
    53   7040.0  1417.836714  1429.083441
    54   7168.0  1427.572226  1434.052485
    55   7296.0  1428.231956  1440.543783
    56   7424.0  1429.370555  1445.957390
    57   7552.0  1428.115109  1452.956976
    58   7680.0  1433.995699  1461.826584
    59   7808.0  1434.452646  1463.904891
    60   7936.0  1440.106887  1465.742401
    61   8064.0  1440.070075  1475.642559
    62   8192.0  1436.544137  1482.850955
    63   8320.0  1390.844800  1404.542462
    64   8448.0  1379.572423  1403.777915
    65   8576.0  1398.247458  1393.654150
    66   8704.0  1387.694518  1398.248332
    67   8832.0  1378.171627  1403.996644
    68   8960.0  1395.786961  1413.944474
    69   9088.0  1410.029512  1415.313797
    70   9216.0  1400.693285  1420.987749
    71   9344.0  1400.155452  1424.334019
    72   9472.0  1398.275887  1432.954674
    73   9600.0  1396.744078  1436.024237
    74   9728.0  1399.799665  1444.629795
    75   9856.0  1418.058945  1441.039915
    76   9984.0  1402.059013  1450.792260
    77  10112.0  1409.712240  1456.288861
    78  10240.0  1414.745975  1469.846482
    79  10368.0  1413.389116  1460.220722
    80  10496.0  1408.363572  1468.273963
    81  10624.0  1411.238702  1468.306139
    82  10752.0  1402.923108  1473.382416
    83  10880.0  1395.897160  1480.502841
    84  11008.0  1420.578000  1475.998247
    85  11136.0  1421.377742  1486.379725
    86  11264.0  1429.324434  1485.156139
    87  11392.0  1418.063257  1489.771288
    88  11520.0  1424.584497  1494.250262
    89  11648.0  1424.460742  1496.316076
    90  11776.0  1430.611434  1504.970066
    91  11904.0  1446.149255  1505.797361
    92  12032.0  1424.150467  1508.381913
    93  12160.0  1421.006373  1511.306646
    94  12288.0  1438.715349  1394.536289
    95  12416.0  1449.881370  1390.794321
    96  12544.0  1439.613711  1396.201991
    97  12672.0  1447.882392  1391.904637




.. GENERATED FROM PYTHON SOURCE LINES 241-245

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 23.340 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
