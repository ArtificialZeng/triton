
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-51

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 52-60

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 62-71

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 71-101

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 102-103

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 103-159

.. code-block:: Python


    device = torch.cuda.current_device()
    properties = driver.active.utils.get_device_properties(device)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software piepling stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel, num_programs = kernels.get(BLOCK_SIZE, (None, 0))
        if kernel is None:
            kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                           num_stages=num_stages, num_warps=num_warps, grid=(1, ))
            kernel._init_handles()
            n_regs = kernel.n_regs
            size_smem = kernel.metadata.shared
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
            occupancy = min(occupancy, SIZE_SMEM // size_smem)
            num_programs = NUM_SM * occupancy
            kernels[BLOCK_SIZE] = (kernel, num_programs)

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](
            y,
            x,
            x.stride(0),
            y.stride(0),
            n_rows,
            n_cols,
        )
        return y









.. GENERATED FROM PYTHON SOURCE LINES 160-162

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 164-166

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 166-173

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device='cuda')
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 174-175

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 177-182

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 182-213

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch'],  # possible values for `line_arg``
            line_names=[
                "Triton",
                "Torch",
            ],  # label name for the lines
            styles=[('blue', '-'), ('green', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device='cuda', dtype=torch.float32)
        stream = torch.cuda.Stream()
        torch.cuda.set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch
    0     256.0   483.373814   691.410530
    1     384.0   618.284228   807.811239
    2     512.0   758.376010   913.515445
    3     640.0   796.139488   952.027425
    4     768.0   884.106445  1017.410462
    5     896.0   931.881621  1060.687548
    6    1024.0   987.890277  1124.106325
    7    1152.0  1109.174905   614.814920
    8    1280.0  1155.811618   669.919401
    9    1408.0  1159.883220   725.626002
    10   1536.0  1195.718508   779.164094
    11   1664.0  1211.134824   814.654418
    12   1792.0  1234.185297   856.258949
    13   1920.0  1254.848750   909.547141
    14   2048.0  1280.527072   960.181979
    15   2176.0  1266.130989   977.655676
    16   2304.0  1269.216999  1008.152209
    17   2432.0  1301.518687  1054.155940
    18   2560.0  1305.882814  1083.215684
    19   2688.0  1309.052134  1103.331214
    20   2816.0  1326.479814  1133.798566
    21   2944.0  1327.003204  1164.515965
    22   3072.0  1355.254207  1182.967957
    23   3200.0  1354.312383  1196.631655
    24   3328.0  1363.235926  1222.992633
    25   3456.0  1374.813321  1247.572000
    26   3584.0  1379.471259  1261.300215
    27   3712.0  1383.468547  1269.207431
    28   3840.0  1390.274965  1303.357204
    29   3968.0  1392.744551  1313.293799
    30   4096.0  1395.432489  1324.162252
    31   4224.0  1330.762484  1160.325788
    32   4352.0  1337.949192  1173.518168
    33   4480.0  1354.147132  1180.916725
    34   4608.0  1364.426067  1194.316176
    35   4736.0  1358.579305  1196.606959
    36   4864.0  1377.283327  1220.686468
    37   4992.0  1373.385271  1235.483630
    38   5120.0  1372.565674  1248.511059
    39   5248.0  1373.629334  1257.727135
    40   5376.0  1374.573949  1286.122332
    41   5504.0  1383.845804  1297.084539
    42   5632.0  1383.685094  1312.995012
    43   5760.0  1395.206666  1324.070267
    44   5888.0  1388.315574  1342.564480
    45   6016.0  1398.865022  1352.200223
    46   6144.0  1412.620414  1373.401229
    47   6272.0  1412.668209  1374.286825
    48   6400.0  1419.615232  1385.639329
    49   6528.0  1412.254639  1396.029543
    50   6656.0  1420.756083  1403.343240
    51   6784.0  1414.896258  1413.407734
    52   6912.0  1428.907998  1423.411192
    53   7040.0  1420.585163  1432.429243
    54   7168.0  1427.360996  1432.997128
    55   7296.0  1431.118521  1444.424889
    56   7424.0  1428.966318  1445.439665
    57   7552.0  1423.730409  1454.009783
    58   7680.0  1433.413558  1459.313934
    59   7808.0  1432.827777  1467.103305
    60   7936.0  1434.701316  1467.759825
    61   8064.0  1440.947239  1472.594700
    62   8192.0  1436.460859  1483.730067
    63   8320.0  1389.977920  1403.779373
    64   8448.0  1378.195253  1405.602653
    65   8576.0  1398.062328  1394.030166
    66   8704.0  1390.322779  1401.275831
    67   8832.0  1385.130096  1402.587790
    68   8960.0  1399.843525  1411.445897
    69   9088.0  1410.506406  1416.167489
    70   9216.0  1405.039396  1425.971433
    71   9344.0  1400.395161  1423.667293
    72   9472.0  1396.965321  1432.095267
    73   9600.0  1393.987492  1436.089884
    74   9728.0  1398.630681  1442.419996
    75   9856.0  1415.951782  1442.786485
    76   9984.0  1400.951074  1451.195595
    77  10112.0  1413.151543  1455.469015
    78  10240.0  1419.068719  1466.755375
    79  10368.0  1415.493789  1460.576378
    80  10496.0  1415.119991  1467.882429
    81  10624.0  1412.048253  1467.768703
    82  10752.0  1413.444281  1472.473186
    83  10880.0  1402.165427  1477.500992
    84  11008.0  1422.362282  1479.161425
    85  11136.0  1422.191503  1484.824616
    86  11264.0  1431.432382  1485.935090
    87  11392.0  1417.856347  1491.204361
    88  11520.0  1422.069283  1492.275406
    89  11648.0  1427.668803  1499.136712
    90  11776.0  1433.812357  1501.546145
    91  11904.0  1445.187994  1506.729085
    92  12032.0  1426.405254  1508.512115
    93  12160.0  1419.359808  1512.522930
    94  12288.0  1435.564739  1391.805939
    95  12416.0  1448.499886  1391.218675
    96  12544.0  1441.379527  1393.247778
    97  12672.0  1449.838003  1391.809115




.. GENERATED FROM PYTHON SOURCE LINES 214-218

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 28.602 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
