
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-51

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 52-60

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 62-71

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 71-101

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 102-103

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 103-159

.. code-block:: Python


    device = torch.cuda.current_device()
    properties = driver.active.utils.get_device_properties(device)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software piepling stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel, num_programs = kernels.get(BLOCK_SIZE, (None, 0))
        if kernel is None:
            kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                           num_stages=num_stages, num_warps=num_warps, grid=(1, ))
            kernel._init_handles()
            n_regs = kernel.n_regs
            size_smem = kernel.metadata.shared
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
            occupancy = min(occupancy, SIZE_SMEM // size_smem)
            num_programs = NUM_SM * occupancy
            kernels[BLOCK_SIZE] = (kernel, num_programs)

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](
            y,
            x,
            x.stride(0),
            y.stride(0),
            n_rows,
            n_cols,
        )
        return y









.. GENERATED FROM PYTHON SOURCE LINES 160-162

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 164-166

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 166-173

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device='cuda')
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 174-175

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 177-182

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 182-213

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch'],  # possible values for `line_arg``
            line_names=[
                "Triton",
                "Torch",
            ],  # label name for the lines
            styles=[('blue', '-'), ('green', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device='cuda', dtype=torch.float32)
        stream = torch.cuda.Stream()
        torch.cuda.set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch
    0     256.0   475.060846   709.074357
    1     384.0   610.635773   819.555965
    2     512.0   761.835939   916.651232
    3     640.0   795.645740   960.254719
    4     768.0   873.072626  1033.844023
    5     896.0   934.739198  1063.927283
    6    1024.0   989.182084  1124.382272
    7    1152.0  1102.163829   614.774188
    8    1280.0  1143.852210   670.234528
    9    1408.0  1165.090589   726.419758
    10   1536.0  1192.329344   783.453540
    11   1664.0  1211.500627   815.727086
    12   1792.0  1241.274608   856.822975
    13   1920.0  1254.734358   909.549982
    14   2048.0  1280.626661   960.572117
    15   2176.0  1260.094733   976.466126
    16   2304.0  1266.292805  1008.420489
    17   2432.0  1299.398935  1054.775818
    18   2560.0  1306.913884  1083.993895
    19   2688.0  1313.132607  1102.154770
    20   2816.0  1319.767516  1133.682431
    21   2944.0  1324.541242  1165.608961
    22   3072.0  1345.948862  1187.319134
    23   3200.0  1353.317951  1197.073288
    24   3328.0  1362.184035  1226.517558
    25   3456.0  1376.569801  1252.502384
    26   3584.0  1375.539016  1258.804183
    27   3712.0  1381.362517  1271.446114
    28   3840.0  1389.529161  1299.599028
    29   3968.0  1393.583910  1313.476068
    30   4096.0  1401.222749  1322.823129
    31   4224.0  1339.581698  1157.686686
    32   4352.0  1334.034010  1172.081576
    33   4480.0  1357.275985  1184.793415
    34   4608.0  1367.252480  1191.189115
    35   4736.0  1360.230134  1198.440679
    36   4864.0  1376.884590  1220.398302
    37   4992.0  1371.360324  1237.960749
    38   5120.0  1377.359466  1249.741407
    39   5248.0  1381.754294  1261.187144
    40   5376.0  1374.865214  1285.005493
    41   5504.0  1379.677536  1301.533568
    42   5632.0  1390.656194  1311.818589
    43   5760.0  1391.414127  1326.741575
    44   5888.0  1389.162063  1341.948060
    45   6016.0  1397.740289  1352.687180
    46   6144.0  1406.735112  1375.376464
    47   6272.0  1415.802520  1377.310445
    48   6400.0  1412.626180  1386.389112
    49   6528.0  1413.471797  1392.730459
    50   6656.0  1422.010102  1404.966312
    51   6784.0  1414.076807  1416.383618
    52   6912.0  1424.627652  1424.732746
    53   7040.0  1420.134534  1434.476580
    54   7168.0  1427.444110  1433.025789
    55   7296.0  1431.238781  1443.324683
    56   7424.0  1429.881715  1444.168053
    57   7552.0  1428.307824  1452.841433
    58   7680.0  1434.263134  1457.764707
    59   7808.0  1431.195013  1466.070081
    60   7936.0  1436.508197  1468.753995
    61   8064.0  1436.517142  1473.055873
    62   8192.0  1441.712649  1486.763035
    63   8320.0  1390.625019  1403.305944
    64   8448.0  1380.844776  1403.799767
    65   8576.0  1393.572072  1398.489051
    66   8704.0  1388.079859  1401.836290
    67   8832.0  1383.452582  1404.604860
    68   8960.0  1395.091205  1413.510427
    69   9088.0  1407.515734  1420.497176
    70   9216.0  1403.619519  1425.920264
    71   9344.0  1401.414839  1426.952575
    72   9472.0  1400.364074  1435.561355
    73   9600.0  1394.697916  1435.009922
    74   9728.0  1400.229567  1441.288302
    75   9856.0  1414.901886  1438.672455
    76   9984.0  1401.534796  1454.827727
    77  10112.0  1415.074321  1452.550586
    78  10240.0  1417.970044  1467.803625
    79  10368.0  1413.801922  1461.768742
    80  10496.0  1415.416347  1466.664436
    81  10624.0  1411.941579  1468.744053
    82  10752.0  1403.337615  1470.681781
    83  10880.0  1399.271947  1480.831715
    84  11008.0  1420.171454  1475.184573
    85  11136.0  1422.598486  1487.737685
    86  11264.0  1427.339305  1485.081877
    87  11392.0  1409.043413  1491.755444
    88  11520.0  1424.341321  1495.131988
    89  11648.0  1428.486342  1496.455946
    90  11776.0  1434.325708  1502.324224
    91  11904.0  1442.814122  1506.403761
    92  12032.0  1421.535036  1506.218896
    93  12160.0  1419.286582  1515.319205
    94  12288.0  1433.786729  1391.413773
    95  12416.0  1449.521900  1390.131803
    96  12544.0  1441.871965  1393.908025
    97  12672.0  1451.677137  1395.524205




.. GENERATED FROM PYTHON SOURCE LINES 214-218

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 26.433 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
