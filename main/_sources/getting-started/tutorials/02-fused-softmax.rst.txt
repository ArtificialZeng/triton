
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-51

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 52-60

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 62-71

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 71-101

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 102-103

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 103-159

.. code-block:: Python


    device = torch.cuda.current_device()
    properties = driver.active.utils.get_device_properties(device)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software piepling stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel, num_programs = kernels.get(BLOCK_SIZE, (None, 0))
        if kernel is None:
            kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                           num_stages=num_stages, num_warps=num_warps, grid=(1, ))
            kernel._init_handles()
            n_regs = kernel.n_regs
            size_smem = kernel.metadata.shared
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
            occupancy = min(occupancy, SIZE_SMEM // size_smem)
            num_programs = NUM_SM * occupancy
            kernels[BLOCK_SIZE] = (kernel, num_programs)

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](
            y,
            x,
            x.stride(0),
            y.stride(0),
            n_rows,
            n_cols,
        )
        return y









.. GENERATED FROM PYTHON SOURCE LINES 160-162

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 164-166

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 166-173

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device='cuda')
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 174-175

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 177-182

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 182-213

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch'],  # possible values for `line_arg``
            line_names=[
                "Triton",
                "Torch",
            ],  # label name for the lines
            styles=[('blue', '-'), ('green', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device='cuda', dtype=torch.float32)
        stream = torch.cuda.Stream()
        torch.cuda.set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch
    0     256.0   475.581977   708.619322
    1     384.0   619.872425   812.799315
    2     512.0   752.326527   927.222924
    3     640.0   788.217790   946.386719
    4     768.0   880.887679  1014.912158
    5     896.0   937.344158  1074.519017
    6    1024.0   994.049328  1120.599053
    7    1152.0  1096.160464   616.484209
    8    1280.0  1136.037680   669.424776
    9    1408.0  1150.661622   725.262518
    10   1536.0  1195.385896   783.556680
    11   1664.0  1218.037815   812.802866
    12   1792.0  1240.453775   857.206087
    13   1920.0  1249.594057   910.379759
    14   2048.0  1281.002942   960.369226
    15   2176.0  1258.141618   976.327061
    16   2304.0  1268.029374  1013.671493
    17   2432.0  1295.384387  1059.587886
    18   2560.0  1306.614187  1084.683454
    19   2688.0  1317.169033  1104.769558
    20   2816.0  1327.217578  1127.015242
    21   2944.0  1321.850846  1164.100210
    22   3072.0  1351.140419  1185.534776
    23   3200.0  1355.270950  1195.189132
    24   3328.0  1350.926797  1219.700403
    25   3456.0  1370.851095  1249.846232
    26   3584.0  1370.733345  1257.045186
    27   3712.0  1380.222691  1272.332674
    28   3840.0  1386.847005  1304.931759
    29   3968.0  1390.096765  1314.800917
    30   4096.0  1395.691852  1329.296474
    31   4224.0  1336.892109  1157.837774
    32   4352.0  1338.490269  1173.375508
    33   4480.0  1350.203136  1183.423201
    34   4608.0  1361.692557  1198.281856
    35   4736.0  1359.538511  1196.113447
    36   4864.0  1374.159725  1224.748171
    37   4992.0  1370.339012  1237.542346
    38   5120.0  1371.061881  1250.239195
    39   5248.0  1373.741013  1256.002531
    40   5376.0  1382.862170  1286.354639
    41   5504.0  1377.679797  1300.142739
    42   5632.0  1378.558008  1311.940458
    43   5760.0  1393.962179  1329.921162
    44   5888.0  1395.824888  1346.085280
    45   6016.0  1401.488037  1355.059080
    46   6144.0  1406.345907  1374.157489
    47   6272.0  1412.687019  1376.883517
    48   6400.0  1415.309106  1389.410912
    49   6528.0  1417.204727  1392.583463
    50   6656.0  1422.082775  1405.407043
    51   6784.0  1416.999653  1415.459830
    52   6912.0  1427.997548  1424.580919
    53   7040.0  1420.079821  1433.713238
    54   7168.0  1428.226868  1434.182051
    55   7296.0  1426.907241  1443.570904
    56   7424.0  1431.245969  1444.524696
    57   7552.0  1429.852775  1455.236120
    58   7680.0  1438.222846  1459.114601
    59   7808.0  1432.084205  1467.194446
    60   7936.0  1435.612336  1467.986631
    61   8064.0  1434.118461  1472.734245
    62   8192.0  1442.312192  1483.740088
    63   8320.0  1388.784296  1401.371945
    64   8448.0  1380.648971  1407.791889
    65   8576.0  1397.384833  1396.228603
    66   8704.0  1393.329000  1400.798649
    67   8832.0  1382.315605  1401.590681
    68   8960.0  1396.830311  1413.000037
    69   9088.0  1409.916407  1418.336829
    70   9216.0  1406.385628  1423.454382
    71   9344.0  1399.874528  1424.049331
    72   9472.0  1399.237655  1435.753027
    73   9600.0  1397.459628  1430.972090
    74   9728.0  1397.891660  1440.957731
    75   9856.0  1413.115159  1443.096680
    76   9984.0  1403.193995  1448.557274
    77  10112.0  1410.619129  1460.444766
    78  10240.0  1419.479137  1469.013209
    79  10368.0  1410.951646  1462.658968
    80  10496.0  1418.695729  1464.967769
    81  10624.0  1408.428065  1471.324774
    82  10752.0  1406.421698  1472.142310
    83  10880.0  1400.046054  1480.109648
    84  11008.0  1420.714401  1480.192162
    85  11136.0  1419.632677  1486.624982
    86  11264.0  1431.403761  1485.485510
    87  11392.0  1414.297153  1487.808132
    88  11520.0  1424.387130  1492.888886
    89  11648.0  1420.436500  1499.220597
    90  11776.0  1426.911001  1499.184103
    91  11904.0  1443.713303  1507.713718
    92  12032.0  1425.647437  1507.984222
    93  12160.0  1419.542002  1513.501788
    94  12288.0  1433.414196  1389.778836
    95  12416.0  1444.076917  1390.692027
    96  12544.0  1441.321385  1394.654073
    97  12672.0  1443.668416  1391.940930




.. GENERATED FROM PYTHON SOURCE LINES 214-218

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 25.339 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
